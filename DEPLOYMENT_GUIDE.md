# AstroSense Deployment & Usage Guide

## ðŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose 20.10+
- NASA API Key (get from https://api.nasa.gov/)
- Cesium Ion Token (get from https://cesium.com/ion/)
- Domain name and SSL certificates (for production)

---

## ðŸ“¦ Installation

### Option 1: Development Deployment

1. **Clone and configure environment:**
```bash
git clone <repository-url>
cd astrosense

# Set up environment variables
cp backend/.env.example backend/.env
cp frontend/.env.example frontend/.env.local

# Edit backend/.env with your NASA API key
nano backend/.env
```

2. **Start all services:**
```bash
docker-compose up -d
```

3. **Access the system:**
- Backend API: http://localhost:8000
- Frontend Dashboard: http://localhost:3000
- API Documentation: http://localhost:8000/docs

### Option 2: Production Deployment (Recommended)

1. **Server Requirements:**
   - Linux server (Ubuntu 20.04+ recommended)
   - 4+ CPU cores
   - 8+ GB RAM
   - 100+ GB SSD storage
   - Docker & Docker Compose installed

2. **Clone and configure:**
```bash
git clone <repository-url>
cd astrosense

# Copy production environment template
cp .env.production.example .env.production

# Edit with your production values
nano .env.production
```

3. **Configure SSL certificates (recommended):**
```bash
# Create SSL directory
mkdir -p nginx/ssl

# Copy your SSL certificates
cp your-cert.pem nginx/ssl/cert.pem
cp your-key.pem nginx/ssl/key.pem

# Update nginx.conf to enable SSL (uncomment SSL sections)
nano nginx/nginx.conf
```

4. **Deploy production stack:**
```bash
# Build and start production services
docker-compose -f docker-compose.prod.yml up -d

# Check service status
docker-compose -f docker-compose.prod.yml ps

# View logs
docker-compose -f docker-compose.prod.yml logs -f
```

5. **Access production system:**
- Frontend: https://yourdomain.com
- API: https://yourdomain.com/api
- Monitoring: https://yourdomain.com:3001 (Grafana)
- Metrics: https://yourdomain.com:9090 (Prometheus)

### Option 2: Manual Installation

#### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys and database credentials

# Initialize database
psql -U postgres -c "CREATE DATABASE astrosense_db;"
psql -U postgres -d astrosense_db -f database/schema.sql

# Run the backend
python main.py
```

#### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Configure environment
cp .env.example .env.local
# Edit .env.local with backend URL

# Run development server
npm run dev
```

---

## ðŸŽ¯ System Components

### 1. Data Ingestion Layer
**Location:** `backend/services/api_client.py`

Fetches real-time space weather data from:
- NASA DONKI API (CME events, solar flares)
- NOAA SWPC (solar wind, Bz field, Kp-index)

**Features:**
- Exponential backoff retry (3 attempts)
- 60-second response caching
- Rate limiting (100ms between requests)
- Comprehensive error handling

**Usage:**
```python
from services.api_client import api_client

# Fetch all space weather data
data = await api_client.fetch_all_space_weather_data()

# Fetch specific data
cme_events = await api_client.fetch_donki_cme_events()
solar_wind = await api_client.fetch_noaa_solar_wind()
mag_field = await api_client.fetch_noaa_mag_field()
```

### 2. Data Processing Layer

#### Validation Engine
**Location:** `backend/services/validation.py`

Validates data quality and completeness.

```python
from services.validation import validation_engine

# Validate a single record
is_valid = validation_engine.validate_record(data, "space_weather_data")

# Check quality metrics
metrics = validation_engine.get_quality_metrics()
print(f"Completeness: {metrics['completeness_percentage']}%")

# Check 90% threshold
meets_threshold = validation_engine.check_quality_threshold(90.0)
```

#### Normalization Engine
**Location:** `backend/services/normalization.py`

Normalizes features to [0, 1] range.

```python
from services.normalization import normalization_engine

# Normalize space weather data
normalized = normalization_engine.normalize_space_weather_data(raw_data)

# Encode flare class
encoded = normalization_engine.encode_flare_class("X2.5")  # Returns 5.25

# Impute missing values
imputed = normalization_engine.impute_missing("solar_wind_speed", lookback_hours=6)
```

#### Feature Extraction Engine
**Location:** `backend/services/feature_extraction.py`

Extracts 12-dimensional feature vectors.

```python
from services.feature_extraction import feature_extractor

# Extract features
feature_vector = feature_extractor.extract_features(normalized_data)
# Returns: numpy array of shape (12,)

# Get feature names
names = feature_extractor.get_feature_names()
# ['solar_wind_speed_norm', 'bz_field_norm', 'kp_index_norm', ...]
```

### 3. ML Model Training

#### Synthetic Data Generator
**Location:** `backend/ml/synthetic_data_generator.py`

Generates training data with synthetic anomalies.

```python
from ml.synthetic_data_generator import SyntheticDataGenerator

generator = SyntheticDataGenerator()

# Generate complete dataset
features, labels = generator.generate_training_dataset(
    normal_samples=500,
    moderate_samples=300,
    severe_samples=150,
    anomaly_samples=50
)

# Create train/val/test splits
X_train, X_val, X_test, y_train, y_val, y_test = generator.create_train_val_test_split(
    features, labels
)
```

#### Model Trainer
**Location:** `backend/ml/model_trainer.py`

Trains Random Forest models.

```python
from ml.model_trainer import ModelTrainer

trainer = ModelTrainer()

# Create and train model
trainer.create_model(n_estimators=100, max_depth=20)
metrics = trainer.train(X_train, y_train, X_val, y_val)

# Cross-validation
cv_results = trainer.cross_validate(X_train, y_train, cv=5)

# Save model
model_path = trainer.save_model(version="1.0.0", metrics=metrics)

# Load model
trainer.load_model(model_path)

# Make predictions
predictions = trainer.predict(X_test)
```

### 4. Physics Rules Engine
**Location:** `backend/services/physics_rules.py`

Applies physics-based prediction rules.

```python
from services.physics_rules import physics_engine

# Predict impacts using physics rules
predictions = physics_engine.predict_impacts({
    'bz': -15.0,
    'solar_wind_speed': 650.0,
    'cme_speed': 1200.0,
    'kp_index': 7.0,
    'flare_class': 'X2.5'
})

# Check for immediate blackout
blackout = physics_engine.check_flare_blackout('X2.5')  # Returns True
```

### 5. Fusion Combiner
**Location:** `backend/services/fusion_combiner.py`

Combines ML and physics predictions (60/40 weighting).

```python
from services.fusion_combiner import fusion_combiner

# Combine predictions
combined = fusion_combiner.combine_predictions(
    ml_predictions={'aviation': 75.0, 'telecom': 60.0},
    physics_predictions={'aviation': 85.0, 'telecom': 55.0}
)
# Result: {'aviation': 79.0, 'telecom': 58.0}

# Resolve conflicts
result = fusion_combiner.fuse_with_conflict_resolution(
    ml_predictions, physics_predictions, conflict_threshold=20.0
)

# Get discrepancy summary
summary = fusion_combiner.get_discrepancy_summary()
```

---

## ðŸ”§ Production Configuration

### Environment Variables Setup

#### Required Variables
```bash
# Database
POSTGRES_USER=astrosense_prod
POSTGRES_PASSWORD=<secure-password>
POSTGRES_DB=astrosense_prod

# Redis
REDIS_PASSWORD=<secure-password>

# API Keys
NASA_DONKI_API_KEY=<your-nasa-key>
NEXT_PUBLIC_CESIUM_ION_TOKEN=<your-cesium-token>

# Security
SECRET_KEY=<32-character-secure-key>

# Domain Configuration
CORS_ORIGINS=https://yourdomain.com
NEXT_PUBLIC_API_URL=https://yourdomain.com/api
NEXT_PUBLIC_WS_URL=wss://yourdomain.com/api/stream
```

#### Optional Variables
```bash
# Monitoring
GRAFANA_PASSWORD=<secure-password>
ENABLE_MONITORING=true

# SSL
NEXT_PUBLIC_CSP_NONCE=<random-nonce>

# Performance
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=30
```

### Database Migration

1. **Initial setup:**
```bash
# Create database backup directory
mkdir -p backups

# Initialize database with schema
docker-compose -f docker-compose.prod.yml exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -f /docker-entrypoint-initdb.d/schema.sql
```

2. **Backup and restore:**
```bash
# Create backup
docker-compose -f docker-compose.prod.yml exec postgres pg_dump -U $POSTGRES_USER $POSTGRES_DB > backups/backup_$(date +%Y%m%d_%H%M%S).sql

# Restore from backup
docker-compose -f docker-compose.prod.yml exec -T postgres psql -U $POSTGRES_USER -d $POSTGRES_DB < backups/backup_file.sql
```

### SSL Certificate Setup

1. **Using Let's Encrypt (recommended):**
```bash
# Install certbot
sudo apt-get update
sudo apt-get install certbot

# Obtain certificates
sudo certbot certonly --standalone -d yourdomain.com

# Copy certificates
sudo cp /etc/letsencrypt/live/yourdomain.com/fullchain.pem nginx/ssl/cert.pem
sudo cp /etc/letsencrypt/live/yourdomain.com/privkey.pem nginx/ssl/key.pem

# Set proper permissions
sudo chown $USER:$USER nginx/ssl/*.pem
chmod 600 nginx/ssl/*.pem
```

2. **Auto-renewal setup:**
```bash
# Add to crontab
echo "0 12 * * * /usr/bin/certbot renew --quiet && docker-compose -f docker-compose.prod.yml restart nginx" | sudo crontab -
```

### Firewall Configuration

```bash
# Allow HTTP and HTTPS
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# Allow SSH (if needed)
sudo ufw allow 22/tcp

# Allow monitoring ports (restrict to your IP)
sudo ufw allow from YOUR_IP_ADDRESS to any port 3001
sudo ufw allow from YOUR_IP_ADDRESS to any port 9090

# Enable firewall
sudo ufw enable
```

---

## ðŸ“Š Monitoring and Maintenance

### Health Checks

1. **Application health:**
```bash
# Check all services
curl -f http://localhost/health

# Check specific components
curl -f http://localhost:8000/health  # Backend
curl -f http://localhost:3000/api/health  # Frontend
```

2. **Database health:**
```bash
docker-compose -f docker-compose.prod.yml exec postgres pg_isready -U $POSTGRES_USER
```

3. **Redis health:**
```bash
docker-compose -f docker-compose.prod.yml exec redis redis-cli --no-auth-warning -a $REDIS_PASSWORD ping
```

### Log Management

1. **View logs:**
```bash
# All services
docker-compose -f docker-compose.prod.yml logs -f

# Specific service
docker-compose -f docker-compose.prod.yml logs -f backend
docker-compose -f docker-compose.prod.yml logs -f frontend
docker-compose -f docker-compose.prod.yml logs -f nginx
```

2. **Log rotation (automatic):**
   - Configured in docker-compose.prod.yml
   - Max 10MB per file, 3 files retained
   - Logs automatically rotated by Docker

### Performance Monitoring

1. **Access Grafana dashboard:**
   - URL: https://yourdomain.com:3001
   - Username: admin
   - Password: (from GRAFANA_PASSWORD env var)

2. **Key metrics to monitor:**
   - CPU usage (< 80%)
   - Memory usage (< 85%)
   - Disk usage (< 90%)
   - Response times (< 2 seconds)
   - Error rates (< 5%)
   - Database connections

3. **Prometheus metrics:**
   - URL: https://yourdomain.com:9090
   - Query examples:
     ```
     cpu_percent
     memory_percent
     request_count
     error_count
     avg_response_time
     ```

### Backup Strategy

1. **Automated backups:**
```bash
# Create backup script
cat > backup.sh << 'EOF'
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
docker-compose -f docker-compose.prod.yml exec -T postgres pg_dump -U $POSTGRES_USER $POSTGRES_DB > backups/astrosense_$DATE.sql
gzip backups/astrosense_$DATE.sql

# Keep only last 30 days
find backups/ -name "*.sql.gz" -mtime +30 -delete
EOF

chmod +x backup.sh

# Schedule daily backups
echo "0 2 * * * /path/to/astrosense/backup.sh" | crontab -
```

2. **Model backups:**
```bash
# Backup ML models
tar -czf backups/ml_models_$(date +%Y%m%d).tar.gz ml_models/
```

---

## ðŸ”„ Updates and Maintenance

### Application Updates

1. **Update application:**
```bash
# Pull latest code
git pull origin main

# Rebuild and restart services
docker-compose -f docker-compose.prod.yml build --no-cache
docker-compose -f docker-compose.prod.yml up -d

# Verify deployment
docker-compose -f docker-compose.prod.yml ps
```

2. **Rolling updates (zero downtime):**
```bash
# Update backend only
docker-compose -f docker-compose.prod.yml up -d --no-deps backend

# Update frontend only
docker-compose -f docker-compose.prod.yml up -d --no-deps frontend
```

### Database Maintenance

1. **Vacuum and analyze:**
```bash
docker-compose -f docker-compose.prod.yml exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "VACUUM ANALYZE;"
```

2. **Check database size:**
```bash
docker-compose -f docker-compose.prod.yml exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT pg_size_pretty(pg_database_size('$POSTGRES_DB'));"
```

### System Maintenance

1. **Clean up Docker:**
```bash
# Remove unused images
docker image prune -f

# Remove unused volumes
docker volume prune -f

# Remove unused networks
docker network prune -f
```

2. **Update system packages:**
```bash
sudo apt-get update
sudo apt-get upgrade -y
sudo apt-get autoremove -y
```

---

## ðŸš¨ Troubleshooting

### Common Issues

#### 1. Service Won't Start
```bash
# Check service status
docker-compose -f docker-compose.prod.yml ps

# Check logs for errors
docker-compose -f docker-compose.prod.yml logs service-name

# Check resource usage
docker stats

# Restart specific service
docker-compose -f docker-compose.prod.yml restart service-name
```

#### 2. Database Connection Issues
```bash
# Check database is running
docker-compose -f docker-compose.prod.yml ps postgres

# Test connection
docker-compose -f docker-compose.prod.yml exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT version();"

# Check connection pool
curl -s http://localhost:8000/health | jq '.database'
```

#### 3. High Memory Usage
```bash
# Check memory usage by service
docker stats --no-stream

# Restart services if needed
docker-compose -f docker-compose.prod.yml restart

# Check for memory leaks in logs
docker-compose -f docker-compose.prod.yml logs backend | grep -i "memory\|oom"
```

#### 4. SSL Certificate Issues
```bash
# Check certificate validity
openssl x509 -in nginx/ssl/cert.pem -text -noout

# Test SSL connection
openssl s_client -connect yourdomain.com:443 -servername yourdomain.com

# Renew Let's Encrypt certificate
sudo certbot renew --dry-run
```

#### 5. API Rate Limiting
```bash
# Check rate limit status
curl -I http://localhost/api/fetch-data

# Adjust rate limits in nginx.conf
nano nginx/nginx.conf
# Look for: limit_req zone=api burst=20 nodelay;

# Restart nginx
docker-compose -f docker-compose.prod.yml restart nginx
```

### Performance Issues

#### 1. Slow Response Times
```bash
# Check system resources
htop
df -h
iostat -x 1

# Check database performance
docker-compose -f docker-compose.prod.yml exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT * FROM pg_stat_activity WHERE state = 'active';"

# Check slow queries
docker-compose -f docker-compose.prod.yml logs backend | grep -i "slow\|timeout"
```

#### 2. High CPU Usage
```bash
# Identify CPU-intensive processes
docker stats --no-stream

# Check application metrics
curl -s http://localhost:8000/health | jq '.system_metrics'

# Scale services if needed (add more workers)
# Edit docker-compose.prod.yml and increase replicas
```

### Recovery Procedures

#### 1. Complete System Recovery
```bash
# Stop all services
docker-compose -f docker-compose.prod.yml down

# Restore from backup
docker-compose -f docker-compose.prod.yml up -d postgres redis
sleep 30
docker-compose -f docker-compose.prod.yml exec -T postgres psql -U $POSTGRES_USER -d $POSTGRES_DB < backups/latest_backup.sql

# Start remaining services
docker-compose -f docker-compose.prod.yml up -d
```

#### 2. Database Recovery
```bash
# Stop backend to prevent writes
docker-compose -f docker-compose.prod.yml stop backend

# Restore database
docker-compose -f docker-compose.prod.yml exec -T postgres psql -U $POSTGRES_USER -d $POSTGRES_DB < backups/backup_file.sql

# Restart backend
docker-compose -f docker-compose.prod.yml start backend
```

---

## ðŸ“ž Support and Maintenance

### Monitoring Alerts

Set up alerts for critical metrics:

1. **Disk space > 90%**
2. **Memory usage > 85%**
3. **CPU usage > 80% for 5+ minutes**
4. **Error rate > 5%**
5. **Response time > 2 seconds**
6. **Service downtime**

### Maintenance Schedule

**Daily:**
- Check service health
- Review error logs
- Monitor resource usage

**Weekly:**
- Review performance metrics
- Check backup integrity
- Update security patches

**Monthly:**
- Database maintenance (VACUUM, ANALYZE)
- Log cleanup
- Security audit
- Performance optimization review

### Emergency Contacts

- System Administrator: [contact-info]
- Database Administrator: [contact-info]
- Security Team: [contact-info]
- On-call Engineer: [contact-info]

---

## ðŸ§ª Running Tests

### All Tests
```bash
cd backend
pytest tests/ -v
```

### Property-Based Tests Only
```bash
pytest tests/ -v -m property
```

### Specific Test Files
```bash
pytest tests/test_api_client_properties.py -v
pytest tests/test_validation_properties.py -v
pytest tests/test_normalization_properties.py -v
pytest tests/test_feature_extraction_properties.py -v
pytest tests/test_ml_properties.py -v
pytest tests/test_physics_properties.py -v
```

### Test Coverage
```bash
pytest tests/ --cov=services --cov=ml --cov-report=html
```

---

## ðŸ“Š Complete Workflow Example

```python
import asyncio
from services.api_client import api_client
from services.validation import validation_engine
from services.normalization import normalization_engine
from services.feature_extraction import feature_extractor
from ml.model_trainer import ModelTrainer
from services.physics_rules import physics_engine
from services.fusion_combiner import fusion_combiner

async def predict_space_weather_impacts():
    # 1. Fetch data
    data = await api_client.fetch_all_space_weather_data()
    
    # 2. Validate
    solar_wind = data['solar_wind']
    if not validation_engine.validate_record(solar_wind, 'solar_wind'):
        print("Validation failed!")
        return
    
    # 3. Normalize
    normalized = normalization_engine.normalize_space_weather_data(solar_wind)
    
    # 4. Extract features
    features = feature_extractor.extract_features(normalized)
    
    # 5. ML prediction
    trainer = ModelTrainer()
    trainer.load_model('./ml_models/random_forest_v1.0.0.pkl')
    ml_pred = trainer.predict(features.reshape(1, -1))
    
    # 6. Physics prediction
    physics_pred = physics_engine.predict_impacts(solar_wind)
    
    # 7. Fuse predictions
    final_pred = fusion_combiner.combine_predictions(
        {'aviation': ml_pred[0][0], 'telecom': ml_pred[0][1]},
        {'aviation': physics_pred['aviation_hf_blackout'], 
         'telecom': physics_pred['telecom_degradation']}
    )
    
    print(f"Final Predictions: {final_pred}")
    return final_pred

# Run
asyncio.run(predict_space_weather_impacts())
```

---

## ðŸ”§ Configuration

### Backend Environment Variables
```env
# NASA DONKI API
NASA_DONKI_API_KEY=your_api_key_here
NASA_DONKI_BASE_URL=https://api.nasa.gov/DONKI

# NOAA SWPC
NOAA_SWPC_BASE_URL=https://services.swpc.noaa.gov/json

# Database
DATABASE_URL=postgresql://astrosense:password@localhost:5432/astrosense_db

# API
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000

# Cache
CACHE_TTL_SECONDS=60

# Logging
LOG_LEVEL=INFO
```

### Frontend Environment Variables
```env
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_WS_URL=ws://localhost:8000
NEXT_PUBLIC_CESIUM_ION_TOKEN=your_cesium_token_here
```

---

## ðŸ“ˆ Performance Metrics

### System Capabilities
- **Data Ingestion:** < 2 seconds per API call (with caching)
- **Data Validation:** 90%+ completeness threshold
- **Feature Extraction:** 12-dimensional vectors in < 100ms
- **ML Prediction:** < 50ms per prediction
- **Physics Calculation:** < 10ms per prediction
- **Total Pipeline:** < 3 seconds end-to-end

### Test Coverage
- **19 Property-Based Tests** with 100 iterations each
- **Properties Validated:** 1-11, 63-67, 73-77
- **Code Coverage:** Core services fully tested

---

## ðŸ› Troubleshooting

### Common Issues

**1. Database Connection Error**
```bash
# Check PostgreSQL is running
docker ps | grep postgres

# Recreate database
docker-compose down -v
docker-compose up -d
```

**2. API Rate Limiting**
```python
# Increase cache TTL in .env
CACHE_TTL_SECONDS=120
```

**3. Model Not Found**
```bash
# Train a new model
cd backend
python -c "from ml.model_trainer import ModelTrainer; from ml.synthetic_data_generator import SyntheticDataGenerator; g = SyntheticDataGenerator(); f, l = g.generate_training_dataset(); X_train, X_val, X_test, y_train, y_val, y_test = g.create_train_val_test_split(f, l); t = ModelTrainer(); t.create_model(); t.train(X_train, y_train, X_val, y_val); t.save_model('1.0.0')"
```

**4. Import Errors**
```bash
# Ensure you're in the backend directory and venv is activated
cd backend
source venv/bin/activate
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

---

## ðŸ“š API Documentation

Once the backend is running, visit:
- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc

---

## ðŸŽ“ Next Steps

The current implementation (Tasks 1-5) provides:
âœ… Complete data pipeline
âœ… ML training infrastructure
âœ… Physics-based prediction
âœ… Fusion system

**Remaining work (Tasks 6-18):**
- Sector-specific predictors (Aviation, Telecom, GPS, Power, Satellite)
- Composite score calculator
- Alert management system
- Database integration
- REST API endpoints
- WebSocket streaming
- Frontend dashboard with visualizations
- Deployment configuration

---

## ðŸ“ž Support

For issues or questions:
1. Check the troubleshooting section above
2. Review test files for usage examples
3. Consult the design document: `.kiro/specs/astrosense-space-weather/design.md`
4. Review requirements: `.kiro/specs/astrosense-space-weather/requirements.md`

---

## ðŸ“„ License

MIT License - See LICENSE file for details
